{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21bf8d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luca.lucchina\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\luca.lucchina\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import silhouette_score as sil_, calinski_harabasz_score as calinski_\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from datetime import datetime, timezone\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import ipaddress\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter\n",
    "import plotly.express as px\n",
    "import matplotlib.dates as mdates\n",
    "import folium\n",
    "import pytz\n",
    "from timezonefinder import TimezoneFinder\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31b41c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/omnipot.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42771565",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8e468f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "0",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "1107e5ec-0311-47dc-b063-60faf0c04360",
       "rows": [
        [
         "src_port",
         "0"
        ],
        [
         "dst_port",
         "0"
        ],
        [
         "timestamp",
         "0"
        ],
        [
         "src_as",
         "2657"
        ],
        [
         "src_city",
         "5"
        ],
        [
         "src_country",
         "0"
        ],
        [
         "src_countryCode",
         "42"
        ],
        [
         "src_geo_str",
         "0"
        ],
        [
         "src_ip",
         "0"
        ],
        [
         "src_proxy",
         "0"
        ],
        [
         "src_regionName",
         "171"
        ],
        [
         "dst_as",
         "0"
        ],
        [
         "dst_city",
         "0"
        ],
        [
         "dst_country",
         "0"
        ],
        [
         "dst_countryCode",
         "0"
        ],
        [
         "dst_geo_str",
         "0"
        ],
        [
         "dst_ip",
         "0"
        ],
        [
         "dst_proxy",
         "0"
        ],
        [
         "dst_regionName",
         "0"
        ],
        [
         "protocol",
         "0"
        ],
        [
         "payload_4kb_hex",
         "209362"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 21
       }
      },
      "text/plain": [
       "src_port                0\n",
       "dst_port                0\n",
       "timestamp               0\n",
       "src_as               2657\n",
       "src_city                5\n",
       "src_country             0\n",
       "src_countryCode        42\n",
       "src_geo_str             0\n",
       "src_ip                  0\n",
       "src_proxy               0\n",
       "src_regionName        171\n",
       "dst_as                  0\n",
       "dst_city                0\n",
       "dst_country             0\n",
       "dst_countryCode         0\n",
       "dst_geo_str             0\n",
       "dst_ip                  0\n",
       "dst_proxy               0\n",
       "dst_regionName          0\n",
       "protocol                0\n",
       "payload_4kb_hex    209362\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61b74711",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], unit=\"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f6c59d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling payload nan with \"\" the ndrop nan that are not in the payload column as empty payloads are valid\n",
    "df.fillna({\"payload_4kb_hex\":\"\"}, inplace=True)\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d431058d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ip_encoding(ipv4):\n",
    "    oct1, oct2, oct3, oct4 = ipv4.split('.')\n",
    "    return  int(oct1), int(oct2), int(oct3), int(oct4)\n",
    "\n",
    "df[['src_oct1', 'src_oct2', 'src_oct3', 'src_oct4']] = df[\"src_ip\"].apply(get_ip_encoding).apply(pd.Series)\n",
    "df[['dst_oct1', 'dst_oct2', 'dst_oct3', 'dst_oct4']] = df[\"dst_ip\"].apply(get_ip_encoding).apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0206db9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"src_geo_str\"] = df[\"src_geo_str\"].str.split(\"|\").apply(lambda lst: [float(x) for x in lst])\n",
    "df[\"dst_geo_str\"] = df[\"dst_geo_str\"].str.split(\"|\").apply(lambda lst: [float(x) for x in lst])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49e12bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "def get_payload_features(payload):\n",
    "    default = {'length': 0, 'unique_bytes': 0, 'entropy': 0.0,\n",
    "               'mean_byte': 0.0, 'std_byte': 0.0}\n",
    "    if not isinstance(payload, str) or len(payload) == 0:\n",
    "        return default\n",
    "    try:\n",
    "        data = bytes.fromhex(payload)\n",
    "    except ValueError:\n",
    "        return default\n",
    "    length = len(data)\n",
    "    if length == 0:\n",
    "        return default\n",
    "    arr = np.frombuffer(data, dtype=np.uint8)\n",
    "    counts = np.bincount(arr, minlength=256)\n",
    "    probs = counts / length\n",
    "\n",
    "    probs = probs[probs > 0]\n",
    "    ent = entropy(probs, base=2)\n",
    "    mean = float(arr.mean())\n",
    "    std = float(arr.std())\n",
    "    return {\n",
    "        'length': length,\n",
    "        'unique_bytes': int(np.count_nonzero(counts)),\n",
    "        'entropy': float(ent),\n",
    "        'mean_byte': mean,\n",
    "        'std_byte': std\n",
    "    }\n",
    "\n",
    "payload_feats = ['pl_length', 'pl_unique_bytes', 'pl_entropy', 'pl_mean_byte', 'pl_std_byte']\n",
    "df[payload_feats] = df[\"payload_4kb_hex\"].apply(get_payload_features).apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b702fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def decode_payload(hex_str: str, protocol: str) -> str:\n",
    "    if not hex_str or not isinstance(hex_str, str):\n",
    "        return \"\"\n",
    "    \n",
    "    try:\n",
    "        raw_bytes = bytes.fromhex(hex_str)\n",
    "        if not raw_bytes:\n",
    "            return \"\"\n",
    "\n",
    "        # SSH \n",
    "        if protocol == 'ssh':\n",
    "            # Clean SSH banners\n",
    "            if raw_bytes.startswith(b'SSH-'):\n",
    "                banner = raw_bytes.decode('utf-8', errors='ignore').split('\\n')[0]\n",
    "                return banner.strip()\n",
    "            \n",
    "            # Detect HTTP requests on SSH port\n",
    "            if raw_bytes.startswith((b'GET ', b'POST ', b'HEAD ', b'PUT ')):\n",
    "                try:\n",
    "                    return \"HTTP_OVER_SSH: \" + raw_bytes.decode('utf-8').split('\\r\\n')[0]\n",
    "                except:\n",
    "                    return \"HTTP_OVER_SSH_BINARY\"\n",
    "            \n",
    "            # Detect binary commands\n",
    "            try:\n",
    "                text = raw_bytes.decode('utf-8', errors='ignore')\n",
    "                if any(cmd in text.lower() for cmd in ['wget', 'curl', 'chmod', 'sh -c']):\n",
    "                    return \"SSH_CMD: \" + ' '.join(text.split())[:200]\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            return f\"SSH_BINARY_{len(raw_bytes)}B\"\n",
    "\n",
    "        # HTTP/HTTPS\n",
    "        elif protocol in ['http', 'https']:\n",
    "            # Reject SSH banners in HTTP\n",
    "            if raw_bytes.startswith(b'SSH-'):\n",
    "                return \"SSH_OVER_HTTP\"\n",
    "            \n",
    "            # Handle valid HTTP\n",
    "            if raw_bytes.startswith((b'GET ', b'POST ', b'HEAD ', b'PUT ')):\n",
    "                try:\n",
    "                    first_line = raw_bytes.decode('utf-8').split('\\r\\n')[0]\n",
    "                    return f\"HTTP: {first_line}\"\n",
    "                except:\n",
    "                    return \"HTTP_BINARY\"\n",
    "            \n",
    "            # Handle SSL/TLS\n",
    "            if len(raw_bytes) > 0 and raw_bytes[0] == 0x16:\n",
    "                return \"SSL_HANDSHAKE\"\n",
    "            \n",
    "            # Other cases\n",
    "            try:\n",
    "                text = raw_bytes.decode('utf-8', errors='ignore')\n",
    "                if text.strip():\n",
    "                    return f\"HTTP_TEXT: {text[:200]}\"\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            return f\"HTTP_BINARY_{len(raw_bytes)}B\"\n",
    "\n",
    "        # SMB \n",
    "        elif protocol == 'smb':\n",
    "            if len(raw_bytes) > 8:\n",
    "                # SMB command mapping\n",
    "                smb_commands = {\n",
    "                    0x00: \"SMB_NEGOTIATE\",\n",
    "                    0x73: \"SMB_NEGOTIATE\",  # Common variant\n",
    "                    0x75: \"SMB_TREE_CONNECT\",\n",
    "                    0x1d: \"SMB_SESSION_SETUP\",\n",
    "                    0x25: \"SMB_CREATE\",\n",
    "                    0x2e: \"SMB_IOCTL\",\n",
    "                    0x32: \"SMB_READ\",\n",
    "                    0x0b: \"SMB_TREE_DISCONNECT\"\n",
    "                }\n",
    "                cmd = smb_commands.get(raw_bytes[4], f\"SMB_UNKNOWN_{raw_bytes[4]}\")\n",
    "                return cmd if cmd != \"SMB_NEGOTIATE\" or raw_bytes[4:8] == b'\\xffSMB' else \"SMB_INVALID\"\n",
    "            return \"SMB_EMPTY\"\n",
    "\n",
    "        # Other Protocols \n",
    "        else:\n",
    "            try:\n",
    "                text = raw_bytes.decode('utf-8', errors='ignore')\n",
    "                if text.strip():\n",
    "                    return f\"{protocol}_TEXT: {text[:200]}\"\n",
    "            except:\n",
    "                pass\n",
    "            return f\"{protocol}_BINARY_{len(raw_bytes)}B\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"DECODE_ERROR_{str(e)}\"\n",
    "\n",
    "# Reapply decoding\n",
    "df['payload_decoded'] = df.apply(\n",
    "    lambda row: decode_payload(row['payload_4kb_hex'], row['protocol']), \n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f31421",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luca.lucchina\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\luca.lucchina\\.cache\\huggingface\\hub\\models--microsoft--codebert-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_code_embeddings(payloads, model_name='microsoft/codebert-base', batch_size=16):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)\n",
    "\n",
    "    embeddings = []\n",
    "    for i in range(0, len(payloads), batch_size):\n",
    "        batch = payloads[i:i + batch_size]\n",
    "        inputs = tokenizer(batch, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            cls_vectors = outputs.last_hidden_state[:, 0, :]\n",
    "            embeddings.extend(cls_vectors.cpu().numpy())\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "# Usage\n",
    "payloads = df['payload_decoded'].astype(str).tolist()\n",
    "vectors = get_code_embeddings(payloads)\n",
    "cosine_sim_matrix = cosine_similarity(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4af9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdbscan import HDBSCAN\n",
    "clusterer = HDBSCAN(min_cluster_size=10000).fit(vectors)\n",
    "df['payload_cluster'] = clusterer.labels_\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "reduced = PCA(n_components=2).fit_transform(vectors)\n",
    "plt.scatter(reduced[:,0], reduced[:,1], c=df['payload_cluster'], cmap='tab10')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ed880b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only payloads that are long enough and have some entropy\n",
    "df_anom = df[(df['pl_length'] > 10) & (df['pl_entropy'] > 1.0)]\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "clf = IsolationForest(contamination=0.01, random_state=42)\n",
    "clf.fit([vectors[i] for i in df_anom.index])\n",
    "df_anom['anomaly_score'] = clf.decision_function([vectors[i] for i in df_anom.index])\n",
    "df_anom['is_anomaly'] = clf.predict([vectors[i] for i in df_anom.index]) == -1\n",
    "\n",
    "df.loc[df_anom.index, 'anomaly_score'] = df_anom['anomaly_score']\n",
    "df.loc[df_anom.index, 'is_anomaly'] = df_anom['is_anomaly']\n",
    "df['is_anomaly'] = df['is_anomaly'].fillna(False)  # others default to not anomalous\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94d156e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
